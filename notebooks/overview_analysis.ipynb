{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('../venv/venv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve database connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the connection string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dbapi(cls):\n",
    " import psycopg2\n",
    " return psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "connection_string = f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "engine = create_engine(connection_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT * FROM xdr_data'\n",
    "data = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data.select_dtypes(include=['float64']).columns\n",
    "text_columns = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "data_cleaned = data.copy()\n",
    "\n",
    "data_cleaned[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
    "data_cleaned[text_columns] = data[text_columns].fillna('N/A')\n",
    "\n",
    "# Remove rows with empty 'MSISDN/Number'\n",
    "data_cleaned = data_cleaned.dropna(subset=['MSISDN/Number'])\n",
    "\n",
    "display(data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the cleaned data to xdr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'SELECT * FROM xdr_data'\n",
    "data = pd.read_sql(query, engine)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 Handsets Used by Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_handsets = data['Handset Type'].value_counts().head(10)\n",
    "print(\"Top 10 Handsets:\")\n",
    "print(top_10_handsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 3 Handset Manufacturers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_manufacturers = data['Handset Manufacturer'].value_counts().head(3)\n",
    "print(\"Top 3 Handset Manufacturers:\")\n",
    "print(top_3_manufacturers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5 Handsets Per Top 3 Manufacturers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for manufacturer in top_3_manufacturers.index:\n",
    "    top_5_handsets = data[data['Handset Manufacturer'] == manufacturer]['Handset Type'].value_counts().head(5)\n",
    "    print(f\"\\nTop 5 Handsets for Manufacturer {manufacturer}:\")\n",
    "    print(top_5_handsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by each user (assuming 'MSISDN/Number' is the identifier for users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DL and UL columns\n",
    "dl_columns = [\n",
    "    'Social Media DL (Bytes)', \n",
    "    'Google DL (Bytes)', \n",
    "    'Email DL (Bytes)', \n",
    "    'Youtube DL (Bytes)', \n",
    "    'Netflix DL (Bytes)', \n",
    "    'Gaming DL (Bytes)', \n",
    "    'Other DL (Bytes)'\n",
    "]\n",
    "\n",
    "ul_columns = [\n",
    "    'Social Media UL (Bytes)', \n",
    "    'Google UL (Bytes)', \n",
    "    'Email UL (Bytes)', \n",
    "    'Youtube UL (Bytes)', \n",
    "    'Netflix UL (Bytes)', \n",
    "    'Gaming UL (Bytes)', \n",
    "    'Other UL (Bytes)'\n",
    "]\n",
    "\n",
    "# Group by each user using 'MSISDN/Number' as the identifier\n",
    "user_overview = data.groupby('MSISDN/Number').agg(\n",
    "    xdr_sessions=('Dur. (ms)', 'count'),  # Number of xDR sessions\n",
    "    total_duration=('Dur. (ms)', 'sum'),  # Total session duration\n",
    "    **{col: (col, 'sum') for col in dl_columns},  # Sum each DL column\n",
    "    **{col: (col, 'sum') for col in ul_columns}   # Sum each UL column\n",
    ").reset_index()\n",
    "\n",
    "# Sum the DL and UL columns across each group\n",
    "user_overview['total_dl_data'] = user_overview[dl_columns].sum(axis=1)\n",
    "user_overview['total_ul_data'] = user_overview[ul_columns].sum(axis=1)\n",
    "\n",
    "# Calculate total data volume\n",
    "user_overview['total_data_volume'] = user_overview[dl_columns].sum(axis=1) + user_overview[ul_columns].sum(axis=1)\n",
    "\n",
    "# Drop the intermediary columns\n",
    "user_overview = user_overview.drop(columns=dl_columns + ul_columns)\n",
    "\n",
    "print(user_overview.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe data\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate total data (DL + UL)\n",
    "data['total_data'] = data[\n",
    "    ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)', \n",
    "     'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)', 'Other DL (Bytes)',\n",
    "     'Social Media UL (Bytes)', 'Google UL (Bytes)', 'Email UL (Bytes)', \n",
    "     'Youtube UL (Bytes)', 'Netflix UL (Bytes)', 'Gaming UL (Bytes)', 'Other UL (Bytes)']\n",
    "].sum(axis=1)\n",
    "\n",
    "# Segment into deciles based on total duration, dropping duplicate bin edges\n",
    "data['duration_decile'] = pd.qcut(data['Dur. (ms)'], 10, labels=False, duplicates='drop')\n",
    "\n",
    "# Compute total data per decile class\n",
    "total_data_per_decile = data.groupby('duration_decile')['total_data'].sum()\n",
    "\n",
    "print(total_data_per_decile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean, median, etc.\n",
    "metrics = data[['Dur. (ms)', 'total_data']].agg(['mean', 'median', 'std'])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Graphical Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispersion parameters\n",
    "dispersion = data[['Dur. (ms)', 'total_data']].agg(['var', 'std'])\n",
    "print(dispersion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "data[['Dur. (ms)', 'total_data']].hist(bins=30, figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "sns.scatterplot(x='Dur. (ms)', y='total_data', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(data[['total_data', 'Dur. (ms)']])\n",
    "plt.scatter(principal_components[:, 0], principal_components[:, 1])\n",
    "plt.title('PCA of Total Data and Duration')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
